---
title: 'Evaluating the utility of graph similarity metrics in quantitative read-across: A tutorial'
author:
  - name: Brett Hagan
    affiliations:
      - id: orau
        name: ORAU
        department: Oak Ridge Associated Universities
        city: Oak Ridge
        state: TN
        postal-code: 'TN, USA'
      - id: ccte
        name: Center for Computational Toxicology and Exposure (CCTE)
        department: 'Office of Research and Development, US Environment Protection Agency'
        address: 109 TW Alexander Dr
        city: Research Triangle Park
        postal-code: NC 27711 USA
    attributes:
      corresponding: false
  - name: Louis Groff
    affiliations:
      - ref: ccte
  - name: Imran Shah
    affiliations:
      - ref: ccte
  - name: Grace Patlewicz
    affilations:
      - ref: ccte
    attributes:
      corresponding: true
      email: patlewicz.grace@epa.gov
abstract: |
  Read-across is a technique used to fill data gaps for substances lacking specific hazard data. The technique relies on identifying source analogues with relevant data that are ‘similar’ to the substance of interest (the target). Typically, source analogues are identified on the basis of structural similarity but the evaluation of their suitability for read-across depends on other contexts of similarity including their physical property information, chemical reactivity, bioactivity and metabolism. Whilst quantifying structural similarity is well established, often relying on chemical fingerprints and using a similarity index such as Tanimoto to limit the number of analogues returned, characterising other aspects of similarity objectively remains a challenge. Many different aspects of a substance and its associated properties lend themselves to being represented by graphs which offers new opportunities in how analogues could be potentially identified and evaluated for read-across purposes. This manuscript considered at least three such methods; graph kernel, graph embedding, and deep learning (DL) approaches. Each approach was described in brief together with illustrative examples to demonstrate how they could be applied in a read-across context.
keywords:
  - Read-Across
  - Graph similarity
  - Graph kernels
  - Graph convolutional networks (GCNs)
date: last-modified
bibliography: bibliography.bib
mainfont: Comic Sans MS
linestretch: 1.5
format:
  html:
    default
  elsevier-pdf:
    header-includes:
      - '\usepackage{lineno}'
      - '\usepackage{caption}'
    keep-tex: true
    journal:
      name: Elsevier
      formatting: preprint
      model: 3p
      cite-style: super
  docx:
    number-sections: true
execute:
  echo: false
jupyter: pytorch
---

# Introduction

## Background to Read-Across {#sec-background}

There are tens of thousands of substances that exist in active commerce e.g. the US Toxic Substances Control Act (TSCA) comprises \~42,000 substances, of which only a small proportion have undergone sufficient toxicological evaluation. In a recent EPA report [@etap_2024], only 15% of substances in US commerce had been subjected to any of the standard toxicity tests used to characterise human health. Assessing each chemical in turn presents a significant and impractical challenge in terms of cost, animal welfare, and resources [@nrc_toxicity_1984]. *In vitro* and *in silico* approaches have the potential to play a large role in both prioritising which chemicals to focus on and evaluate in the absence of conventional toxicity data. *In silico* approaches encompass (quantitative) structure-activity relationships ((Q)SAR) as well as read-across, both of which relate chemical structure to (eco)toxicological or physical property endpoints. QSARs are often used to address gaps for environmental fate, ecotoxicological and physical property endpoints whereas read-across is most commonly used for human health related endpoints. Notably read-across is cited as the most commonly used adaptation to address information requirements under the European Union's Registration Evaluation and Authorisation of Chemicals (REACH) regulation [@eu_regulation_2006; @macmillan_last_2024].

In brief, read-across describes the method for filling a data gap whereby a substance with existing data (termed the 'source analogue') is used to make a prediction of the same property for a 'target' substance with limited available empirical data. The approach relies on the premise that both source and target substances are 'similar' in some context with relevant information pertaining to a specific outcome [@enoch_chemical_2010; @oecd_guidance_2014]. Key to this approach is the characterisation of similarity. Although structural similarity is arguably the most common approach used to identify candidate source analogues, other similarity contexts such as similarity in physicochemical properties, metabolism, chemical reactivity, bioactivity and toxicological profile also play a significant role in justifying the relevance and suitability of those source analogues for read-across. For example, metabolic similarity might entail an assessment of the similarity of transformation pathways or the commonality of metabolites formed as determined in experimental studies. Physicochemical similarity might compare certain physical property information such as the log of the octanol-water partition coefficient (logKow), melting point, boiling point etc. of source analogues relative to the target substance to determine whether physical form and partitioning are likely to be the same. Similarity in toxicity might evaluate whether the available empirical data identifies the same target organs impacted and whether the potencies are comparable or follow a specific trend. Such similarity context assessments are largely qualitative and heavily reliant on expert judgement in concert with empirical data [@patlewicz_building_2015]. This does result in challenges in terms of reproducibility, scalability and acceptance for regulatory purposes [@shah_systematically_2016]. Indeed, read-across as a technique has been in use for well over 20 years, but acceptance for certain regulatory contexts (e.g. risk assessment) or within specific jurisdictions still remains variable [@patlewicz_towards_2023]. Thus, progress towards approaches that may increase confidence in and reduce the levels of inherent uncertainty in read-across predictions continues to be an area of ongoing research.

Significant effort has been directed towards the evaluation of confidence in analogue identification and evaluation across a wide range of studies [@patlewicz_building_2015; @blackburn_framework_2014; @schultz_assessing_2019; @wu_framework_2010; @patlewicz_navigating_2018]. Several have aimed to define frameworks for characterising uncertainty [@schultz_assessing_2019; @schultz_strategy_2015; @blackburn_framework_2014; @patlewicz_navigating_2018], whereas others have demonstrated how high-throughput screening data can be helpful in substantiating mechanistic or biological similarity within read-across justifications [@escher_towards_2019; @patlewicz_navigating_2018; @rovida_nam-supported_2021]. The European Chemicals Agency (ECHA) have developed a read-across assessment framework in an effort to improve the characterisation and documentation of read-across uncertainties [@european_chemicals_agency_read-across_2017] whereas the Organisation of Economic and Co-operative Development (OECD) have been facilitating the development of case studies with the aim of updating existing grouping technical guidance [@oecd_guidance_2014] with one focus being on reducing read-across uncertainties [@OECD_IATA]. In our own work, Generalised Read-Across (GenRA) [@shah_systematically_2016; @patlewicz_towards_2023] was created with the goals of quantifying performance and uncertainty by establishing performance baselines and quantifying the contribution that different similarity contexts play in identifying of source analogues or making toxicity predictions. Research has continued to systematically evaluate the impact that different types of similarity play in read-across for the prediction of *in vivo* toxicity outcomes [@patlewicz_systematic_2024; @tate_repeat-dose_2021; @helman_extending_2018; @nelms_mechanistic_2018; @boyce_comparing_2022] along with implementing the insights learned into the GenRA (www.comptox.gov/genra) web application [@patlewicz_towards_2023; @shah_genra_2024].

## Source analogue identification

There are a number of software tools that facilitate the identification of source analogues. Most of these use structural similarity as a basis to return analogues. This is usually performed in one of two main ways - either by a descriptor-based similarity calculation or a substructure-based assessment [@kunimoto_maximum_2016]. In practice, this means that a software tool contains a large database (or dataset) of chemical substances which serves as a source analogue inventory. To identify analogues, a search query is then performed using the target substance of interest to return candidate analogues. In a substructure-based approach, a determination of the substructures shared with the target substance are made or matched molecular pairs [@oboyle_using_2014; @noauthor_matched_2021] are generated to identify common core structures that are distinguished at a given site. Such substructure-based calculations are binary - either the target and source analogues share a pre-defined substructure or not, therefore no adjustable threshold exists to tune the returned set of candidate analogues. On the otherhand, the hits returned are often chemical intuitive and interpretable.

In a descriptor-based approach, the key considerations are how the substances forming the source inventory are represented numerically and what metric is used to quantify a specific threshold of similarity. Source analogues can be characterized by 1D, 2D or 3D representations of structures or hybrids of these. The EPA CompTox Chemicals Dashboard [@williams_comptox_2017], [PubChem](https://pubchem.ncbi.nlm.nih.gov/), as well as the many functionalities within the OECD Toolbox (qsartoolbox.org) [@schultz_oecd_2018] facilitate such analogue searches. Two dimensional binary chemical fingerprints are frequently used for practical efficiency especially when a source inventory contains large numbers of substances e.g. 1 million substances. A target substance will then be converted into the same chemical fingerprint representation and a query based on pairwise similarities will return a number of candidates based on the similarity threshold set. The similarity threshold is a quantitative measure between 0 and 1 that summarises the commonality in structure based on the presence and absence of particular chemical fingerprints. By far, the most common similarity index that is used in the Tanimoto (Jaccard) index [@bajusz_why_2015] though there are a number of other similarity indices that can also be used [@bajusz_why_2015; @floris_generalizable_2014]. The choice of similarity index largely depends on the chemical representation used. A Tanimoto index lends itself to binary fingerprints whereas other metrics e.g. cosine would be suitable for a set of continuous descriptors representing the source analogues.

There are several types of chemical fingerprints, one of the most popular the extended connectivity fingerprint (ECFP) or Morgan fingerprint [@rogers_extended-connectivity_2010]. The ECFP defines molecular features by assigning identifiers to each of the atoms in the molecule based on some combination of properties such as atomic number, atomic mass etc. Then each atom collects its identifier and those of its neighbouring atoms into an array and uses a hash function to reduce the array into a single integer identifier. This captures the neighbourhood of the atom. Once all atoms have generated their new identifiers, these are updated and the process is performed several times over. After each iteration, the identifier contains information about the immediate neighbours and then the neighbours of those neighbours and so on until each atom will contain information from all parts of the molecule. Finally the identifiers are converted into a bit array depending on the length of the fingerprint array that the user has defined. ECFP4 is probably the most common ECFP fingerprint where the 4 denotes the largest possible fragment having a width of 4 bonds.

Other fingerprints are termed key or dictionary fingerprints where there is a defined fixed set of substructural features representing molecular characteristics. MACCS (Molecular Access System by Molecular Design Limited) [@durant_reoptimization_2002] and ChemoType ToxPrints [@yang_new_2015] are examples of these. The MACCS fingerprint was one of the first developed, containing 166 structural features. The original ToxPrints comprised a set of 729 generic structural fragments organised by atom, bond, chain, ring types as well as specific chemical groups. Atom pairs is another example where an algorithm of atom typing is performed such that certain values for each atom of a molecule is computed [@carhart_atom_1985]. An atom pair is defined in terms of the atomic environments of, and the shortest path separations between, all pairs of atoms in the topological representation of a chemical structure. In each case, the fingerprint is usually represented as a bit string or binary vector to denote presence and absence of a structural feature that can then be used as a query to search for source analogues. Some fingerprints can encode counts to capture the occurrence count of a structural features rather than the presence or absence of certain fragments.

Chemical fingerprints have proved useful for fast similarity comparisons as well as inputs into development of QSARs for different activity outcomes in particular toxicity endpoints. Regardless of the fingerprint type, there are always limitations in how a chemical may be represented to capture the aspects important for toxicity prediction. For example, the Morgan circular fingerprints are typically poor at perceiving global features of a molecule (e.g. size or shape) and may fail to discriminate between subtle changes between 2 small molecules. One particular issue with using Morgan fingerprints is that whilst they reflect which substructures are present in a molecule, their interconnectedness (particularly over large distances) is lost. More details on the different types of structural representations can be found in a recent review [@Banerjee_2024].

Chemical fingerprints provide a convenient and efficient means to identify potential source analogues on the basis of structural similarity. The fingerprints themselves represent a simplified representation of a chemical that may be insufficient to resolve differences in toxicity outcomes that is important in read-across.

This study took inspiration from the work in Mellor et al (2019) who compared different molecular chemical fingerprints in terms of their pairwise similarities, to evaluate whether considering the inherent representation of a chemical structure as a molecular graph, with atoms as nodes and bonds as edges, might offer novel ways of characterising structural information and in turn similarity for read-across.

## Topological indices

Of course, it important to acknowledge that considering chemicals as molecular graphs is not a novel concept. In fact, a wide variety of chemical properties and processes have been modelled using information derived from molecular graphs for many decades. Traditional topological indices for chemical structures have been in use for more than 45 years. These are algebraic invariants of hydrogen depleted molecular graphs which represent the topology of a molecule. There are hundreds of topological indices but the majority can be broadly categorised into 5 main types namely: degree-based indices, distance-based indices, count-based indices, eigenvalue-based indices and information-theoretic indices [@Alameri_2021].

Degree indices are based on the degree of the nodes in the graph. Historically, the Zagreb Index which is based on the degrees of the nodes focusing on the sum of squares or products of node degrees was the first degree based structural descriptor though developed for a different purpose as described by Gutman [@gutman_2013]. The first true degree based topological index was put forward by Randic in 1975 [@randic_1975]. The so-named Randic Index is defined by the sum of the inverse of the square roots of the degrees of adjacent nodes. It is possibly one of the most widely applied topological indices in chemistry. Randic noted good correlation between the index and a number of physicochemical properties of alkanes such as surface area, boiling point. More information on the Randic index can be found in the following review by Li and Shi [@li_2008].

The most common distance based index is the Wiener Index, which represents the sum of the shortest-path distances between all pairs of nodes in the graph. Wiener demonstrated a correlation between the index and boiling points of alkanes [@wiener_1947]. Count based indices include the Hosoya Index which counts the number of matching sets in the graph. The Hosoya index was first introduced in 1971 [@Hosoya_1971] demonstrating correlations between boiling points of alkanes. Eigenvalue based indices include the Estrada index which is the sum of the exponential of the eigenvalues of the adjacency matrix. Initially it was used to quantity the degree of folding of long chain molecules such as protein. Gutman et al [@IvanGutman2011] provide an extensive survey on the Estrada index and its applications. Finally information-theoretic indices use concepts from information theory to quantify the distribution of certain properties within the graph. The Shannon entropy, one such example, measures the diversity in the distribution of node degrees. It quantifies the complexity or diversity of a molecule based on the distribution of different atom types within its structure, essentially measuring the "uncertainty" in predicting which type of atom will be found at a given position within the molecule; a higher Shannon entropy indicates a greater variety of atom types and a more complex structure. Information entropy in chemistry has been extensively reviewed in Sabirov and Shepelvich [@doi.org/10.3390/e23101240]

Topological indices have been widely and successfully applied to the quantitative correlation of many different molecular properties notably boiling point, chemical reactivity as well as biological activity [@roy_use_2017;<https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=e0c5a4e71fe31479c4430e40014b62eb983a31cd>]. Although the indices have been used in many QSAR studies, one of their main shortcomings has been a perceived lack of interpretability [@todeschini_chemical_1992].

## Graph Similarity

Topological indices provide a single, composite number that characterises each molecule’s structure. Whilst this approach is advantageous for its simplicity and efficiency, it often compresses complex structural information into one value, which can obscure finer details about specific molecular substructures. To address this, a broader range of graph similarity methods have been developed, allowing more granular analysis. Techniques such as graph edit distance, graph isomorphism, and maximum common subgraph matching offer a way to directly compare molecular structures, identifying subtle differences and shared features that single-index values may overlook [@ullmann_algorithm_1976; @pelillo_replicator_1999; @melnik_similarity_2002; @jeh_simrank_2002; @zager_graph_2008; @koutra_algorithms_2011; @chartrand_graph_1998].

### Graph edit distances using Reduced graphs

Reduced graphs provide a summarised representation of a chemical structure that are produced by collapsing connected atoms into single nodes and forming edges between the nodes in accordance with bonds in the original structure. Reduced graphs have been used in a variety of applications in chemoinformatics ranging from the representation and search of Markush structures to the identification of structure-activity relationships (SARs). There are a number of different graph reduction schemes though each has been devised to address a different purpose [@gillet_similarity_2003; @birchall_reduced_2011; @birchall_training_2006]. Graph reduction schemes have been developed for similarity searching often with the objective of identifying substances with similarity in activity. Various methods have also been developed to quantify the similarity between reduced graphs from fingerprint approaches, graph matching as well as an edit distance method. The edit distance approach quantifies the degree of similarity of 2 reduced graphs based on the number and type of operations needed to convert one graph to the other. One benefit of the edit distance method is the ability to assign different weights to different operations - useful when deriving activity specific weights as evidenced in Birchall et al. [@birchall_training_2006]. However, graph edit distance are computationally expensive unless approximation algorithms are used particularly for larger graphs. Garcia-Hernandez et al. employed graph edit distances to reduced graph representations to estimate the bioactivity of a chemical on the basis of the bioactivity of similar compounds and found better performance than the array representation-based approaches they compared against [@RN25].

### Graph isomorphism

Several foundational questions of chemical similarity analysis have often been framed as graph comparison problems; chemical equivalence may be modeled as a graph isomorphism task, i.e. are two chemical graphs identical (isomorphic)? Another question may be to determine whether some chemical graph is included as part of another chemical. Searching for a specific substructure (e.g. a benzene ring) within another chemical has been modeled as a subgraph isomorphism task. The enumeration of possible chemical structures is closely related to graph enumeration [@RN22]. Graph isomorphism is a test of structural equivalence, wherein two graphs are isomorphic if a structure exists that preserves a one-to-one correspondence between the two graphs sets of nodes and edges.

### Maximum common subgraph

A common need in cheminformatics to the ability to align pairs of molecules together to make a determination of the degree of structural overlap. This is useful when exploring SARs, predicting bioactivity of substances or identifying chemical reaction sites. The degree of overlap between a pair of chemicals can be achieved using maximum common subgraph isomorphism algorithms [@duesbury_maximum_2017; @raymond_maximum_2002]. In cheminformatics, maximum common subgraph isomorphism is usually referred to as identifying the maximum common substructure (MCS). Given two structures, the MCS is the largest substructure common to both. Maximum could be interpreted to imply the maximum number of atoms, number of bonds, number of cycles or even some physical property. There are also variations in how atom and bond equivalency might be defined. However the most common MCS is where all atoms are the same if the element numbers are the same and the bonds are of the same type. There are a range of algorithms that can determine the MCS between pairs of chemicals. Some algorithms and perhaps the most prevalent work on the basis of identifying cliques or maximal cliques. A clique is a set of nodes in a graph such that each node is connected to each and every other node, with a maximal clique in a graph being one that is not contained within another large clique. Examples include the Bron-Kerbosch algorithm [@bron_algorithm_1973] which reports all of the maximal cliques found. TOPSIM [@durand_efficient_1999] is another algorithm designed to find the Maximum Common Edge Subgraph (MCES) between two graphs. The Maximum Common Edge Subgraph is similar to the Maximum Common Subgraph (MCS) problem but focuses on finding the largest subgraph that has the maximum number of edges in common between the two graphs. In this case, the algorithm converts labeled graph representations of two molecules into a compatibility graph. Then a modified maximal clique algorithm is used to find the maximal clique which represents the largest common substructure (excluding common isolated atoms) for the two molecules. A maximal common substructure is obtained by combining the largest common substructure and the common isolated atoms. The size of a maximal common substructure is then used to define both a molecular similarity index and a topological distance for two molecules. Other types of algorithms include subgraph enumeration algorithms which involve enumerating all connected subgraphs common to the two graphs that are being compared and then returning the largest subgraph. Raymond and Willett (2002) reviewed the main solutions for pairwise MCS [@raymond_maximum_2002] including multiple MCS [@dalke_fmcs_2013].

Whilst methods such as the maximum common subgraph (MCS) excel at pinpointing shared structural features in pairwise comparisons, they can become computationally intensive and less scalable when applied to large chemical datasets. To address some of these limitations, more recent graph similarity approaches, such as graph kernel methods, graph embeddings, and deep learning-based techniques, have been developed. Graph kernel methods directly calculate a similarity score between two graphs based on their structural properties. Graph embedding methods transform graphs into numerical representations (vectors) that can be compared using standard distance metrics. Deep learning methods, a subset of graph embedding, learn these numerical representations using neural networks. In particular, graph embeddings and graph kernels enable the transformation of structural information into high-dimensional feature spaces, facilitating rapid comparisons and enabling the discovery of nuanced similarity patterns.

### Graph Kernels

Graph kernels were first introduced as a way to compare complex structures like graphs based on a concept from Haussler’s work on kernels for discrete structures [@kondor_diffusion_2002]. The term “graph kernels” soon emerged to describe methods specifically for comparing graphs [@RN27; @RN45; @scholkopf_fast_2007]. The core idea behind graph kernels is to break down a graph into smaller components, called substructures. These substructures are then used to create feature vectors, which characterise the graph. By comparing these feature vectors, it is possible to measure how similar two graphs are. The inner products of the feature vectors can be efficiently computed to produce a similarity score between the graphs. The key to graph kernels lies in how the graph is decomposed. One simple approach is to count how many node labels are shared between graphs and computing the inner products of these label counts to produce a similarity score [@kriege_survey_2020]. @fig-count provides an example of counting node labels.

![Graph kernel counting node labels.The feature vectors for both graphs are constructed by counting the numbers of node labels in each graph. A similarity score is then obtained by computing the inner products of the feature vectors.](fig-count.png){#fig-count width="100%"}

There are many different ways to decompose a graph in order to compare them. One approach is through random walk kernels. This method involves taking random paths through the graph and counting how often each path occurs in each graph [@gartner_survey_2003]. Shortest path kernels aim to find the shortest paths between labelled nodes (atoms) in each graph and using these to construct feature vectors [@RN30]. A more advanced method builds on the Weisfeiler-Lehman (WL) graph isomorphism heuristic that was introduced by Shervashidze in 2011; known as the WL subtree kernel [@RN46]. The WL isomorphism heuristic works by iteratively updating the labels of each atom based on the labels of its neighbouring atoms. Over several iterations, this process captures more detailed substructures within the molecule. If at any point the labels of the atoms in the molecular graphs do not match, the algorithm is terminated as the two molecular graphs can not be isomorphic. The WL subtree kernel then uses these refined labels to compare different molecular graphs. This method is particularly powerful and has been found to be closely related to operations used in graph neural networks. @fig-wl shows an example iteration of the kernel between two graphs.

![One iteration of the WL kernel. Feature vectors initially consist of counts of original node labels. At each iteration, new labels (colors) are created for each atoms by considering the labels of its neighbours. Nodes a in both G1’ and G2’ are labeled grey as they were both adjacent to a single blue label in the previous iteration, whereas nodes e in G1’ and G2’ are assigned different labels due to the differences in their neighbouring node labels. The feature vectors consist of counts of the original and newly created node labels as iteration continues until a defined limit or convergence is reached. The inner products are computed to obtain a similarity score.](wl.png){#fig-wl width="50%"}

### Graph Embeddings {#sec-graphembed}

Whilst there are numerous advantageous qualities to graph representations, the unstructured, relational nature of the data does not allow it to be directly used as inputs into QSAR models which require numerical data in the form of vectors [@cai_comprehensive_2018]. To overcome this limitation, graph embedding techniques are used to create lower dimensional representations of graph data whilst retaining as much topological and label (or feature) information as possible. Graph embeddings allow for a type of similarity measurement between graphs. Embedding methods represent graphs in a multi-dimensional latent space, where highly similar molecular graphs will lie near each other, whereas molecular dissimilar graphs will lie further apart. The distance between the embedding of two molecular graphs in the latent space provides a quantitative measure of similarity.

A number of different methods exist that are capable of creating graph embeddings which can be broadly divided into two categories: node embeddings, and whole graph embeddings. **Node embeddings** map individual atoms in a molecular graph to numerical vectors, capturing atom characteristics and relationships. **Graph embeddings** on the other hand represent the entire molecular graph as a single vector, often by combining atom embeddings or using other methods, to permit pairwise molecular graph comparisons.There are a variety of different approaches to either task, with well established taxonomies in literature dividing them into three distinct categories; matrix factorization methods, random walk based methods, and neural network methods, with substantial areas of overlap between the three [@xu_understanding_2021; @goyal_graph_2018].

Matrix factorization techniques were the earliest studied, beginning with the multi-dimensional scaling (MDS) that decomposed adjacency matrices [@RN34]. Other factorization methods operate on graph proximity (distance matrices) or graph Laplacian matrices [@RN35; @RN36]. Although factorization methods are the most well-established and theoretically understood, they often scale poorly [@xu_understanding_2020]. Random walk based embeddings [@perozzi_deepwalk_2014] later emerged based upon word and document embedding methodologies such as Word2Vec, adopting the skip-gram neural network model used to create word embeddings to the graph context. The skip-gram model is a simple single hidden layer neural network (see @fig-skip) that is trained to predict the probabilities for each word in a given vocabulary to appear near in sequence to a given target word. The network is trained, and the weights of the trained network are exploited as vectorised word embeddings, with the underlying intuition being that words that often appear in similar contexts are likely highly similar in some context [@mikolov_efficient_2013].

![Skip gram model for Word2Vec word embeddings. A one hidden layer neural network is trained to determine the probabilities for each word in a vocabulary of appearing near in sequence to a given target word. The target word is given as a one-hot encoded input, and after training via backpropagation over a number of epochs, the hidden weights of the network are used as embedded vector representations of words.](survey_skip_gram_figure.png){#fig-skip}

Jaeger et al \[\@[10.1021/acs.jcim.7b00616](https://doi.org/10.1021/acs.jcim.7b00616)\] developed Mol2vec which is synonymous to the concept of Word2Vec but specific to the chemistry domain. Mol2Vec was developed to learn vector representations of molecular substructures that point in similar directions for chemical related substructures. Substuctures were derived using the the Morgan algorithms as "words" and substances as "sentences". The Word2Vec algorithm was then applied to a corpus of 19.9 million substances taken from the ZINC and ChEMBL databases. The feature vectors for the substructures generated were then summed to obtain substance vectors which could be used as inputs for any subsequent machine learning approaches. Zhang et al \[\@[https://pubmed.ncbi.nlm.nih.gov/31998687/https://pubmed.ncbi.nlm.nih.gov/31998687/](https://pubmed.ncbi.nlm.nih.gov/31998687/)\] proposed SPVec, constructed via the combination of SMILES2Vec and ProtVec to represent specific drug-target interactions, where the drug representation was simplified by using SMILES directly. Different from the work by Jaeger et al. (2018), SMILES of drug molecules were used directly rather than generating Morgan substructures as “words” to learn the representations. The approach described in Asgari and Mofrad (2015) was used to train ProtVec here, where protein sequences were regarded as “sentences” and every three non-overlapping amino acids were regarded as a “word.” SMILES2Vec itself was developed by Goh et al \[\@<https://arxiv.org/abs/1712.02034>\] using a deep recurrent neural network (RNN).

DeepWalk adapted the SkipGram approach to a graph setting [@perozzi_deepwalk_2014] for node embedding. Words are analogous to nodes in the graph, the sequences of words (a “context”) are analogous to random walks across node neighborhoods, and the vocabulary of words is analogous to all nodes in the graph. Node2Vec iterated upon DeepWalk with the introduction of parameters to control the length and freedom of the random walk operations [@RN40]. Graph2Vec iterated upon Node2Vec to allow for skip-gram based whole graph embeddings based off rooted subgraphs analogous to words in Word2Vec [@narayanan_graph2vec_2017]. GL2Vec improved upon Graph2Vec in classification tasks by incorporating information gleaned from a line graph representation, better allowing for the capture of structural information [@RN42]. Research has shown that more complicated approaches to graph embeddings may not necessarily result in better performance. The LDP (Local Degree Profile) embedding method was introduced in 2019 and showed comparable performance to more sophisticated embeddings methods while only considering the degree information of nodes in a graphs without considering any label information whatsoever [@RN32].

### Deep Learning Embeddings

Inspired by the widespread success of various deep learning approaches such as convolutional neural networks (CNNs), graph neural networks (GNNs) were introduced in 2009 with the goal of extending existing neural network models for processing graph structured data [@scarselli_graph_2009]. Graph convolutional networks (GCNs) were introduced by Duvenaud (2015) to operate on graphs for molecular property predictions \[\@<https://onlinelibrary.wiley.com/doi/10.1002/qub2.23>\]. Subsequently, Coley et al. constructed feature vectors of atoms using atom and bond attributes in molecules and considered local chemical environment information within different neighborhood radii \[33\]. By directly inputting the complete molecular graph into CNN, the model could learn to recognize atom cluster features, significantly improving the performance of the CNN model. Gilmer et al. reformulated existing models as message passing neural networks (MPNN) and leveraged MPNN to demonstrate state-of-the-art results on quantum mechanical property prediction tasks for small organic molecules \[34\]. For using the geometric structure information in molecules more effectively, Wang et al. used graph structures with convolutional networks to discover the relationship of each atom and designed a convolution spatial graph embedding layer (C-SGEL) to make full use of the spatial connectivity information of molecules \[35\]. They constructed a composite model by stacking multiple C-SGEL and combining them with molecular fingerprints, and achieved the best results on several datasets.

At the base level, GCNs take a graph as input and pass it through a number of convolutional layers that aggregate each nodes neighborhood information. At each training epoch, each node in the graph has its hidden state updated by aggregating each of the node’s neighbors hidden states together by some function, and combining it with the current hidden state of the node. The output of convolutional layers is a set of node embeddings, vectorized representations of each node in the graph. Whole graph embeddings can be generated from these individual node embeddings by combining them in some way through a “pooling” layer that aggregates the node embeddings together. These embeddings can then be used as inputs into different regression or classification based machine learning models.

![Graph convolutional network conceptual model. A graph is given as input into the model and passed through a series of convolutional layers and activation functions that produce embeddings for each node in the graph. The individual node embeddings are aggregated together by some pooling operation in a readout layer in order to produce a whole graph embedding as output that can be used for structured ML tasks such as classification, regression, or prediction.](gcn_model.png){#fig-gcn-concept}

The aim of this study was to compare and contrast different graph based approaches and their utility in assessing similarity for read-across. Morgan chemical fingerprints were used as a baseline comparator. To achieve this objective s Herein, for each family of methods, the approach is described conceptually, and a short, simple example provided to demonstrate how these approach could be practically applied for read-across purposes either for the identification of analogue or to perform an endpoint prediction.

# Methods

## WL Subtree Kernel Example {#sec-wl}

A read-across example, comprising target substance 2-Amino-4,6-dinitrotoluene (2-ADNT) (CASRN 35572-78-2) and its structural analogues, was identified from the published EPA Provisional Peer-Reviewed Toxicity Values (PPRTV) assessments. A PPRTV is defined as a toxicity value derived for use in the EPA Superfund Program. PPRTVs are derived after a review of the relevant scientific literature using established EPA Agency guidance on human health toxicity value derivations. The objective is to provide support for the hazard and dose-response assessment pertaining to chronic and subchronic exposures of substances of concern, to present the major conclusions reached in the hazard identification and derivation of the PPRTVs, and to characterize the overall confidence in these conclusions and toxicity values. Current assessments can be accessed on the U.S. Environmental Protection Agency’s (EPA’s) PPRTV website at https://www.epa.gov/pprtv. In cases where there is a paucity of data to derive a PPRTV for a specific substance, an analogue approach is applied which permits the use of data from related substances to calculate a screening value. The exact procedure is described in more detail in Wang et al [@wang_application_2012].

Five structural analogues with relevant oral non cancer toxicity values were identified for the target substance 2-ADNT (see @tbl-pprtv). Structures were represented as SMILES. WL subtree kernels were derived for the set of candidate analogues to compare their pairwise similarities. Morgan chemical fingerprints (radius = 3, bitvector = 1024) were also derived from which a Jaccard index was calculated. This would provide a similarity metric typically relied upon in analogue searching tools already described. Molecular graph representations were created using the Python package RDKit [@landrum_rdkit]. The open source Python package GraKeL [@siglidis_grakel_2020] was used to implement the WL subtree kernel.

```{python}
import grakel
import matplotlib.pyplot as plt
import grakel
from rdkit import Chem
import numpy as np
import pandas as pd
import networkx as nx
import os
```

```{python}
#| echo: false
dtxsids = ['2-Amino-4,6-Dinitrotoluene',
'2,4,6-Trinitrotoluene',
'2-Methyl-5-nitroaniline',
'Isopropalin',
'Pendimethalin',
'Trifluralin'
]

smiles = ['CC1=C(C=C(C=C1N)[N+]([O-])=O)[N+]([O-])=O',
'CC1=C(C=C(C=C1[N+]([O-])=O)[N+]([O-])=O)[N+]([O-])=O',
'CC1=C(N)C=C(C=C1)[N+]([O-])=O',
'CCCN(CCC)C1=C(C=C(C=C1[N+]([O-])=O)C(C)C)[N+]([O-])=O',
'CCC(CC)NC1=C(C=C(C)C(C)=C1[N+]([O-])=O)[N+]([O-])=O',
'CCCN(CCC)C1=C(C=C(C=C1[N+]([O-])=O)C(F)(F)F)[N+]([O-])=O',

]
```

```{python}
#| echo: false
def smile_to_mol_graph(smile):

    mol = Chem.MolFromSmiles(smile)
    g = nx.Graph()

    for atom in mol.GetAtoms():
        g.add_node(atom.GetIdx(),
                   atom_symbol = atom.GetSymbol())
    # Add edges with bond properties
    for bond in mol.GetBonds():
        g.add_edge(bond.GetBeginAtomIdx(), 
                   bond.GetEndAtomIdx(), 
                   bond_type=str(bond.GetBondType()))

    
    return g
```

```{python}
#| echo: false

graphs = [smile_to_mol_graph(smile) for smile in smiles]
grakel_graphs = grakel.graph_from_networkx(graphs,node_labels_tag='atom_symbol')
```

```{python}
#| echo: false
wl_kernel = grakel.WeisfeilerLehman(base_graph_kernel=grakel.VertexHistogram,normalize=True)
p = wl_kernel.fit_transform(grakel_graphs)
df = pd.DataFrame(p)
df.index = dtxsids
df.columns = dtxsids
```

```{python}
#df
```

```{python}
#[Chem.MolFromSmiles(mol) for mol in smiles][3]
```

```{python}
#[Chem.MolFromSmiles(mol) for mol in smiles][4]
```

## Node Embedding Example {#sec-node2vec}

A dataset of 82 read-across case examples compiled from the literature taken from Patlewicz et al, *in preparation* was used to explore the uility of Node2Vec embeddings in comparing analogue pairs. The 82 cases comprised 468 substances which were converted to graph objects as described in @sec-wl. The Node2Vec python library was used to learn node embeddings by creating biased random walks of the molecular graphs. The embedding dimensions were set to 64, with a random walk length of 30 and parameters to adjust the balance between structural equivalence and homophily.

## Graph Embedding Example {#sec-graph2vec}

To demonstrate the applicability of graph embedding methodologies within a RAx context, an example was developed using a dataset of substances with associated genotoxicity outcomes. The dataset was an updated version of that compiled in Pradeep et al [@pradeep_evaluation_2021] drawn from the EPA Toxicity Values database (ToxValDB). The same methodology as described in Pradeep et al [@pradeep_evaluation_2021] was used to create a dataset with a summary genotoxicity outcome for each chemical. Genotoxicity studies, including *in vitro* and *in vivo* chromosomal aberration, Ames, micronucleus, mouse lymphoma studies were initially retrieved from ToxValDB. To create a single outcome per chemical, the dataset was first grouped by substance identifier and summarized as follows: if a substance was associated with a positive Ames result, a positive genotoxicity outcome was returned, if a substance was not associated with a positive Ames but did have a reported positive chromosomal or micronucleus outcome, it was tagged as a clastogen. If only inconclusive studies were associated with a substance, an inconclusive tag was assigned, finally if only negative outcomes were associated with the substance, a non-genotoxicity outcome was returned. For the dataset compiled with structural information, there were 5403 chemicals with QSAR-READY SMILES and a genotoxicity outcome.

Genotoxicity is an endpoint of particular interest where many different prediction models have been developed from quantitative structure activity relationships (QSARs) to read-across approaches. Examples of models include the Ames mutagenicity model that exists within the EPA TEST suite (see \[https://www.epa.gov/comptox-tools/toxicity-estimation-software-tool-test\] as well as a myriad of genotoxicity models available within the VEGA suite of tools (see \[https://www.vegahub.eu/portfolio-item/vega-qsar/\]. Benigni reviewed the state of the art of modelling genotoxicity [@benigni_data-based_2019] discussing the different approaches that have been applied to which test guidelines have been modelled. There has been a renewed interest in building new models for genotoxicity since the International Conference on Harmonization (ICH) M7 guideline permited the use of *in silico* approaches for predicting Ames mutagenicity for the initial assessment of impurities in pharmaceuticals. The guideline allows for a knowledge base and statistical model to be used in combination to predict Ames mutagenicity. Two modelling challenges were established recently to crowdsource the development of new models to predict the Ames mutagenicity, first of which was reported in Honma et al [@honma_improvement_2019] with a followup study described in Furuhama et al. [@Furuhama_2023].

Beyond QSAR approaches, read-across approaches have been also been applied to genotoxicity as discussed by Benigni [@benigni_towards_2019]. One quantitative read-across approach includes the method employed by GenRA [@shah_systematically_2016], wherein structural aspects of chemicals are used in a K nearest-neighbors (k-nn) classification to derive a similarity weighted activity outcome. Morgan chemical fingerprints form one possible set of structural features to identify similar substances to perform a read-across. An alternative approach is to employ graph representations and embedding methods such as Graph2Vec, GL2Vec, and LDP to characterize substances and identify analogues. Herein, the constructed dataset was used to perform a genotoxicity prediction where the three aforementioned methods were used to create molecular graph representations as a basis to identify similar analogues.

Molecular graph representations were generated using the open source Python package RDKit as described in @sec-wl. Graph2Vec, GL2Vec, and LDP embedding models, implemented within the Python package KarateClub [@karateclub], were used to generate vectorized embeddings for each substance. The embeddings were projected in 2D using a t-distributed stochastic neighborhood embedding (t-SNE) [@van_er_maaten_visualizing_2018], which was color coded by genotoxicity outcome. The embeddings were used as inputs in 2 classifiers; a k-nn classifier and logistic regression to assess their informative content. As a baseline comparator, Morgan chemical fingerprints were used as feature inputs into the same two classifiers. The 2 classifiers were implemented using the open source Python package scikit-learn [@RN53] with the area under the curve-receiver operating characteristic (AUC-ROC) as a performance metric.

## GCN Embeddings Example

The same dataset as described @sec-graph2vec was used to demonstrate the applicability of the GCN embedding method within a RAx context. An end-to-end GCN supervised graph classification model was constructed by using three convolutional layers (GATv2Conv convolutional layer, a graph attentional layer from Brody et al., [@RN56]) with ReLU activation functions, a global mean pooling readout layer, and a single fully connected linear layer to make predictions. For the molecular graphs, one hot encodings of the atom symbol labels were attached as node feature vectors. The graphs were split into a training and validation set of size 4,000 and 1,403 respectively. Using cross entropy loss and an Adam optimizer with a learning rate of 0.001, the model was trained over 50 epochs, with the AUC score of the training and validation graphs reported at each epoch. After training, embeddings for the validation graphs were generated by inputting the graphs into the trained model and extracting the resultant embedding from the readout layer. These were visualized via t-SNE and labeled by outcome. The embeddings were also used as inputs into k-nn and logistic regression classification models, with performance compared against the use of Morgan chemical fingerprints.

# Results

## WL Subtree Kernel

Based on an expert-driven evaluation of the structural, physicochemical, available toxicokinetic (TK) data, and toxicity data, 2,4,6-Trinitrotoluene (TNT) was chosen as the 'best analogue' primarily based on its metabolic similarity, structural similarity, and shared metabolites. The similarity of toxicological outcomes across all the source analogues established confidence in the toxicologic read-across for 2-ADNT. TNT was also determined to be the most health-protective analogue because its point of departure (POD) and corresponding reference dose (RfD) value were lower than the other candidate analogues. WL and Jaccard (based on Morgan fingerprints) pairwise similarities across the target and all analogues are shown in @fig-timeline. TNT had both the highest WL and Jaccard score. The Jaccard similarities based on Morgan fingerprints were notably lower from the WL scores though the ranking in terms of the similarities relative to the target chemical was largely comparable. The difference between nitro group vs. amino group accounted for the slight decrease in WL score from the target 2-ADNT whereas the position of the methyl group appeared not to impact the score. The remaining candidate analogues all had lower WL scores owing to the change of substituent position as well as the substituents themselves. The high WL scores are likely to be as a result of the manner in which the graph was initially constructed using only atom symbols as labels. The absence of other relevant atom properties might better discriminate the differences between the analogues. Substances with similar overall topology but different substituents are likely to yield high WL scores since the WL kernel is sensitive to the global structure of the chemical and may overemphasize this at the expense of distinct local features. To explore this further the manner in which the graphs were constructed (see related code) was refined to incorporate additional atom property information and the WL scores were re-computed.

```{python}
from rdkit.Chem import Descriptors
from rdkit.ML.Descriptors import MoleculeDescriptors
from rdkit.Chem import AllChem
from rdkit.Chem import rdDepictor
from rdkit.Chem.Fingerprints import FingerprintMols
from rdkit import DataStructs

MOLS = dict(zip(dtxsids, smiles))
MOLS = {k:Chem.MolFromSmiles(v) for k,v in MOLS.items()}
MOLS = {i:j for i,j in MOLS.items() if j}
FP0 = pd.DataFrame([np.array(AllChem.GetMorganFingerprintAsBitVect(i,3,1024)) for i in MOLS.values()])
FP0.index = MOLS.keys()
FP0.columns = ['mrgn_%d'%i for i in FP0.columns]
```

```{python}
from scipy.spatial.distance import pdist, squareform
```

```{python}
D_mgrn = pd.DataFrame(squareform(pdist(FP0, 'jaccard')), columns = FP0.index, index = FP0.index)
```

```{python}
S_mgrn = 1-D_mgrn
```

```{python}
#S_mgrn
```

```{python}
#| label: fig-timeline
#| layout-ncol: 2
#| fig-cap: Pairwise similarities based on WL and  Jaccard Morgan Fingerprints

import seaborn as sns
f, ax = plt.subplots(1,2, sharey = True, figsize = (15,10))
sns.heatmap(df, cmap = 'coolwarm', annot = True, ax = ax[0], annot_kws={'size': 6})
sns.heatmap(S_mgrn, cmap = 'coolwarm', annot = True, ax = ax[1], annot_kws={'size': 6});
```

```{python}
#| echo: false
def smile_to_mol_graph(smile):
    mol = Chem.MolFromSmiles(smile)
    g = nx.Graph()
    
    # Add nodes with atom properties
    for atom in mol.GetAtoms():
        node_label = f"{atom.GetSymbol()}_{atom.GetDegree()}_{str(atom.GetHybridization())}_{atom.GetIsAromatic()}"
        g.add_node(atom.GetIdx(), 
                   atom_label=node_label)

    # Add edges with bond properties
    for bond in mol.GetBonds():
        g.add_edge(bond.GetBeginAtomIdx(), 
                   bond.GetEndAtomIdx(), 
                   bond_type=str(bond.GetBondType()))

    return g
```

```{python}
graphs = [smile_to_mol_graph(smile) for smile in smiles]
#grakel_graphs = grakel.graph_from_networkx(graphs,node_labels_tag='atom_symbol')
```

```{python}

# Convert NetworkX graphs to Grakel graphs
grakel_graphs = grakel.graph_from_networkx(graphs, node_labels_tag='atom_label')

# Compute the WL kernel
wl_kernel = grakel.WeisfeilerLehman(base_graph_kernel=grakel.VertexHistogram,normalize=True)
kernel_matrix = wl_kernel.fit_transform(grakel_graphs)

```

```{python}
p1 = pd.DataFrame(kernel_matrix)

p1.index = dtxsids
p1.columns = dtxsids
```

@tbl-pprtv compares the refined WL scores with the original WL based on atom labels alone and the Jaccard metric. Whilst the naive WL scores gave rise to the highest scores, incorporating additional atom property information (including aromaticity, hybridization and atom degree) refines the score (WL-rev) so that the differences between the substituents and their positions are better accounted for. The WL scores offer an effective means of computing a similar score direct from the structural representation bypassing the need to compute separate chemical fingerprints or descriptors. The approach was sensitive to the manner in which the graphs were constructed and care needs to be placed on incorporating atom and bond information so that small local differences between substances such as substituent differences on an aromatic ring structure are not lost in the manner in which global structure is characterized. Careful consideration of the type of information to attribute through label selection is an important aspect of this graph kernel operation.

|        Substance        |   Role    |    DTXSID     |  WL  | Jaccard Morgan FP | WL-rev |
|:-------------:|:----------:|:----------:|:----------:|:----------:|:----------:|
|         2-ADNT          |  Target   | DTXSID6044068 |  1   |         1         |   1    |
|           TNT           | Selected  | DTXSID7024372 | 0.88 |       0.57        |  0.73  |
| 2-Methyl-5-nitroaniline | Candidate | DTXSID4020959 | 0.73 |       0.43        |  0.52  |
|       Isopropalin       | Candidate | DTXSID8024157 | 0.66 |       0.21        |  0.45  |
|      Pendimethalin      | Candidate | DTXSID7024245 | 0.70 |       0.24        |  0.50  |
|       Trifluralin       | Candidate | DTXSID4021395 | 0.62 |       0.23        |  0.42  |

: 2-ADNT is denoted as the target substance based on its role designation. TNT was ultimately selected as the read-across candidate out of the 5 candidate analogues. WL, Jaccard and WL-rev denote the similarity scores computed. WL relies on molecular graphs constructed using only atoms as labels whereas WL-rev scores were derived taking into account other atom property information. The pairwise scores are shown in each case. e.g. TNT was determined to have a Jaccard similarity with 2-ADNT of 0.57 whereas the WL score and revised WL score was 0.88 and 0.73 respectively. {#tbl-pprtv}

## Node Embedding

The cosine distance for selected read-across examples (see @tbl-icf) were calculated on the basis of the node embeddings. On first inspection, the node embeddings appear promising given the low cosine distances - the read-across candidates were found to be similar to their respective targets. However the maximum cosine distance across the overall dataset was determined to be quite low (0.44). Indeed, the pairwise cosine distances based on the embeddings for two random chemicals extracted from the dataset, chlorobenzene and caffeine was computed to be 0.19 which is not that much higher than for their respective candidate analogues. A permutation test between all the individual read-across cases relative to the overall dataset found that 67% of case examples had maximum pairwise distances that were not significantly different. Overall the node embeddings produced did not appear to be able to discriminate read-across case substances that were considered to be particularly similar in terms of their structure relative to the entire dataset suggesting other embeddings that are able to capture the whole graph might be more promising.

```{python}
Dcr = pd.read_csv('/home/grace/Documents/python/metgraph/reports/survey/node2vec.csv', index_col = [0])
```

|      Substance      |   Role    |    DTXSID     | Cosine Distance |     |     |
|:-------------------:|:---------:|:-------------:|:---------------:|:---:|:---:|
|    Chlorobenzene    |  Target   | DTXSID4020298 |        0        |     |     |
| 1,4-Dichlorobenzene | Candidate | DTXSID1020431 |      0.12       |     |     |
| 1,2-Dichlorobenzene | Candidate | DTXSID6020430 |      0.14       |     |     |
|      Caffeine       |  Target   | DTXSID0020232 |        0        |     |     |
|    Theophylline     | Candidate | DTXSID5021336 |      0.14       |     |     |
|     Theobromine     | Candidate | DTXSID9026132 |      0.11       |     |     |

: Selected targets and their candidate analogues and their corresponding pairwise cosine scores. {#tbl-icf}

## Graph Embedding

@fig-graph2vec, @fig-gl2vec and @fig-ldp show the embeddings for Graph2Vec, GL2Vec and LDP as projected into t-SNE plot and color coded by genotoxicity outcome. The mean AUC-ROC scores from 5-fold cross validation classifiers are shown in @tbl-graph2vec.

![Graph2Vec embeddings labeled by genotoxicity outcome](Graph2Vec_250724.png){#fig-graph2vec width="50%" fig-pos="H"}

![GL2Vec embeddings labeled by genotoxicity outcome](GL2Vec_250724.png){#fig-gl2vec width="50%" fig-pos="H"}

![LDP embeddings labeled by genotoxicity outcome](LDP_250724.png){#fig-ldp width="50%" fig-pos="H"}

| Embedding Method | K-nn  | Logistic Regression |
|:----------------:|:-----:|:-------------------:|
|    Morgan FPs    | 0.662 |        0.727        |
|    Graph2Vec     | 0.526 |        0.500        |
|      GL2Vec      | 0.604 |        0.598        |
|       LDP        | 0.596 |        0.549        |

: 5-fold cross validated k-nn and logistic regression genotoxicity classification results using Morgan fingerprints and the three embeddings methods . {#tbl-graph2vec}

The quality of the embeddings generated by Graph2Vec, GL2Vec, or LDP failed to capture relevant chemical features effectively to be able to discriminate between genotoxic and non-genotoxic outcomes. Morgan chemical fingerprints outperformed the graph embeddings using both classifiers. Graph2Vec struggled to separate the data, with almost no discrimination between the two outcomes as shown in @fig-graph2vec. GL2Vec and LDP both provided better discrimination, with clearer clustering of genotoxic and non-genotoxic chemicals (@fig-gl2vec and @fig-ldp). Fine tuning parameters such as embedding length and learning rates may increase performance since all embeddings were generated using the default parameters of the models. Default parameters were also used for the classification models, leaving another area of possible improvement. The graph representations used were also very simple, using only the atom symbol as the node labels. Different types of labels may lead to better performance in embedding methods that rely on node label information alone.

## GCN Embeddings

GCN embeddings were visualized via t-SNE and labeled by outcome as shown in @fig-gcn-nn. The 5-fold cross validation AUC scores for the K-nn and Logistic regression using the GCN embeddings were found to be 0.66 and 0.78 respectively, a comparable performance to Morgan fingerprints using a K-nn approach but a marked improved with the logistic regression.

![GCN embeddings of validation graph set labeled by genotoxicity outcome](GCN_embedding_050824.png){#fig-gcn-nn fig-pos="H"}

A clearer separation between genotoxic and non-genotoxic chemicals in the embedded space created by the supervised classification GCN model was observed (@fig-gcn-nn) with an improvement in both the K-nn and logistic regression classification performance relative to using Morgan fingerprints. As with the previous graph embedding discussed in @sec-graph2vec, default parameters were used for both classification models, likely leaving room for improvement in performance through hyperparameter tuning. As with all DL models, there are a large number of options available when constructing a GCN architecture. Layer types, selection of activation functions, pooling methods, choice of loss functions and optimizers, as well as the fine tuning of parameters such as the optimizer’s learning rate, the number of training epochs and number of neurons per layer are all of significant importance in a network’s performance. Further experimentation with network architecture would likely lead to better performance, but for the purposes of this illustrative example, the application of a generically designed network without any fine tuning was still able to yield reasonable performance.

# Conclusion

In this tutorial review, a selection of approaches to quantifying graph similarity were described and demonstrated for their role in identifying and evaluating analogues within a read-across approach. A WL graph kernel approach was found to be useful in characterizing potential analogues relative to 2-ADNT, identifying TNT as the most similar analogue. TNT was selected as the source analogue for use in the read-across assessment. The WL scores were found to be sensitive to the way in which the graphs were initially constructed such that if atom and bond characteristics were not sufficiently captured, local differences in structural representations could be underrepresented relative to the whole molecular effects and thereby overinflating the resulting scores. Careful attention is needed to capture node and edge information before their use. Both the WL with atom labels or a revised WL taking into account additional atom properties identified TNT as the most similar analogue to 2-ADNT, their relative ranking of analogues was the same even though the actual WL scores differed. The Jaccard scores using Morgan fingerprints were lower no doubt highlighting that small changes in substituents and their positions are not well discriminated across the analogues relative to the target substance. Topological and label information played a significant role in ascertaining the WL similarities.

In contrast embedding approaches building on the Word2Vec approach were found to be poor at capturing relevant molecular information and were ineffective in discriminating between substances that were read-across candidates (using Node2Vec to learn embeddings) or as in the second example were categorized as genotoxic or not. In the latter example, Morgan fingerprints were found to be superior in predicting the genotoxicity outcomes. Graph2Vec and LDP performed particularly poorly whereas GL2Vec was slightly better at discriminating genotoxicity or not using the 2 classifiers.

A Deep learning GCN model fared better, with a marked improvement in performance compared with Morgan fingerprints. Whereas the embedding approaches applied in @sec-graphembed were unsupervised in nature, the GCN required labeled training data to create informed embeddings to facilitate genotoxicity classification. This performance increase observed also came at a cost of resources, and complexity. The GCN approach can be computationally expensive, depending on model parameters, scale of datasets, size of graphs and graph features, and more. These examples illustrate the potential that graph similarity approaches could play significant roles in the identification of suitable analogues for RAx However careful attention needs to be paid to the embedding representation applied and how the initial graphs are constructed.

# Disclaimer {.unnumbered .unnumbered}

This manuscript reflects the opinions of the authors and are not reflective or the opinions or policies of the US EPA.

# References {.unnumbered}

::: {#refs}
:::

{{< pagebreak >}}

```{=tex}
\newpage
\appendix
\renewcommand{\thefigure}{A\arabic{figure}}
\renewcommand{\thetable}{A\arabic{table}}
\setcounter{figure}{0}
\setcounter{table}{0}
```
# Supplementary information {.appendix}

For context, a few of the pertinent terms/definitions associated with graphs are described. A graph $$ G(V, E) $$ is a visual representation of a collection of nodes (also called vertices depending on domain) and edges that connect these nodes providing a structure to represent entities and their relationships. There are three main types of edges that are typically found in a graph: undirected edges, directed edges and weighted edges.

@fig-gg provides an example of an undirected graph with 5 nodes and 5 edges and the corresponding directed graph.

::: {#fig-gg layout-nrow="2"}
![An undirected graph G with 5 nodes and 5 edges](fig-gp.png){#fig-gp width="30%"} ![A directed graph G with 5 nodes and 5 edges](fig-gpd.png){#fig-gpd width="30%"}

Undirected and directed graph with 5 nodes and 5 edges.
:::

Undirected edges are those which identify a connection between nodes but without a given "flow". Directed edges are those where is a clear direction between nodes e.g. within a metabolic graph where a parent chemical transforms into a metabolite. Weighted edges can occur in both directed or undirected edges to depict some quantitative value e.g. the rate of disappearance of parent chemical to its corresponding metabolite.

The neighborhood of a node N(v) is the set of all nodes adjacent to v. In @fig-gg, the neighborhood of node d would be expressed as N(d) = [b,c,e]. A *walk* comprises a sequence of edges and nodes, whereas a *path* is a walk with no repeating nodes visited. In @fig-path, the sequence of nodes [a,b,d] is both a walk and a path in graph G.

![The sequence of nodes \[a,c,d\] is both a walk and a path on G since there are no repeating nodes in the sequence.](fig-path.png){#fig-path width="30%"}

Two graphs are isomorphic if there is a structure that preserves a one-to-one correspondence between the nodes and edges. In other words, if the two graphs differ only by the names of the edges and nodes but are otherwise structurally equivalent, they are said to be isomorphic.